----- nvidia-smi output at Wed 13 Nov 2024 02:34:44 AEDT -----
Package                       Version     Editable project location
----------------------------- ----------- ------------------------------------------------------------------------------------------
accelerate                    1.1.1
aiofiles                      23.2.1
aiohappyeyeballs              2.4.3
aiohttp                       3.10.10
aiosignal                     1.3.1
alabaster                     0.7.16
altair                        5.3.0
annotated-types               0.7.0
antlr4-python3-runtime        4.9.3
anyio                         3.7.1
asttokens                     2.4.1
async-timeout                 4.0.3
attrs                         24.2.0
babel                         2.16.0
bert-score                    0.3.13
bitsandbytes                  0.44.1
certifi                       2024.8.30
charset-normalizer            3.4.0
click                         8.1.7
comm                          0.2.2
contourpy                     1.3.0
cycler                        0.12.1
datasets                      3.0.2
debugpy                       1.8.7
decorator                     4.4.2
decord                        0.6.0
dill                          0.3.8
distro                        1.9.0
docutils                      0.18.1
einops                        0.6.1
einops-exts                   0.0.4
et_xmlfile                    2.0.0
evaluate                      0.4.3
exceptiongroup                1.2.2
executing                     2.1.0
fastapi                       0.103.2
ffmpy                         0.4.0
filelock                      3.16.1
fonttools                     4.54.1
frozenlist                    1.5.0
fsspec                        2024.9.0
ftfy                          6.3.1
gradio                        4.16.0
gradio_client                 0.8.1
h11                           0.14.0
httpcore                      0.17.3
httpx                         0.24.0
huggingface-hub               0.26.1
idna                          3.10
imageio                       2.36.0
imageio-ffmpeg                0.5.1
imagesize                     1.4.1
importlib_metadata            8.5.0
importlib_resources           6.4.5
ipykernel                     6.29.5
ipython                       8.18.1
jedi                          0.19.1
Jinja2                        3.1.4
joblib                        1.4.2
jsonschema                    4.23.0
jsonschema-specifications     2024.10.1
jupyter_client                8.6.3
jupyter_core                  5.7.2
kiwisolver                    1.4.7
latex2mathml                  3.77.0
liger_kernel                  0.3.1
Markdown                      3.7
markdown-it-py                3.0.0
markdown2                     2.5.1
MarkupSafe                    2.1.5
matplotlib                    3.9.2
matplotlib-inline             0.1.7
mdit-py-plugins               0.4.2
mdurl                         0.1.2
model-index                   0.1.11
modelindex                    0.0.2
moviepy                       1.0.3
mpmath                        1.3.0
multidict                     6.1.0
multiprocess                  0.70.16
myst-parser                   3.0.1
narwhals                      1.13.2
nest-asyncio                  1.6.0
networkx                      3.2.1
nltk                          3.9.1
numpy                         1.26.4
nvidia-cublas-cu12            12.1.3.1
nvidia-cuda-cupti-cu12        12.1.105
nvidia-cuda-nvrtc-cu12        12.1.105
nvidia-cuda-runtime-cu12      12.1.105
nvidia-cudnn-cu12             8.9.2.26
nvidia-cufft-cu12             11.0.2.54
nvidia-curand-cu12            10.3.2.106
nvidia-cusolver-cu12          11.4.5.107
nvidia-cusparse-cu12          12.1.0.106
nvidia-nccl-cu12              2.19.3
nvidia-nvjitlink-cu12         12.4.127
nvidia-nvtx-cu12              12.1.105
omegaconf                     2.3.0
open_clip_torch               2.29.0
openai                        1.3.5
opencv-python                 4.10.0.84
openpyxl                      3.1.5
ordered-set                   4.1.0
orjson                        3.10.11
packaging                     24.1
pandas                        2.2.3
parso                         0.8.4
peft                          0.13.2
pexpect                       4.9.0
pillow                        10.4.0
pip                           24.2
platformdirs                  4.3.6
portalocker                   2.10.1
proglog                       0.1.10
prompt_toolkit                3.0.48
propcache                     0.2.0
protobuf                      5.28.3
psutil                        6.1.0
ptyprocess                    0.7.0
pure_eval                     0.2.3
pyarrow                       18.0.0
pycocoevalcap                 1.2
pycocotools                   2.0.8
pydantic                      2.9.2
pydantic_core                 2.23.4
pydub                         0.25.1
Pygments                      2.18.0
pyparsing                     3.2.0
python-dateutil               2.9.0.post0
python-dotenv                 1.0.1
python-multipart              0.0.17
pytorch_sphinx_theme          0.0.24      /srv/scratch/CRUISE/Breeze/Project/Llava/COMP9444_Llava/benchmark/src/pytorch-sphinx-theme
pytz                          2024.2
PyYAML                        6.0.2
pyzmq                         26.2.0
referencing                   0.35.1
regex                         2024.9.11
requests                      2.32.3
rich                          13.9.4
rpds-py                       0.20.1
ruff                          0.7.2
safetensors                   0.4.5
scikit-learn                  1.2.2
scipy                         1.13.1
semantic-version              2.10.0
sentencepiece                 0.1.99
setuptools                    75.1.0
shellingham                   1.5.4
shortuuid                     1.0.13
six                           1.16.0
sniffio                       1.3.1
snowballstemmer               2.2.0
Sphinx                        6.1.3
sphinx-copybutton             0.5.2
sphinx_design                 0.6.1
sphinx-notfound-page          1.0.4
sphinx-tabs                   3.4.7
sphinxcontrib-applehelp       2.0.0
sphinxcontrib-devhelp         2.0.0
sphinxcontrib-htmlhelp        2.1.0
sphinxcontrib-jquery          4.1
sphinxcontrib-jsmath          1.0.1
sphinxcontrib-qthelp          2.0.0
sphinxcontrib-serializinghtml 2.0.0
stack-data                    0.6.3
starlette                     0.27.0
sty                           1.0.6
svgwrite                      1.4.3
sympy                         1.13.1
tabulate                      0.9.0
threadpoolctl                 3.5.0
tiktoken                      0.8.0
timeout-decorator             0.5.0
timm                          0.6.13
tokenizers                    0.20.3
tomlkit                       0.12.0
toolz                         1.0.0
torch                         2.2.0
torchvision                   0.17.0
tornado                       6.4.1
tqdm                          4.66.5
traitlets                     5.14.3
transformers                  4.46.2
triton                        2.2.0
typer                         0.12.5
typing_extensions             4.12.2
tzdata                        2024.2
urllib3                       2.2.3
uvicorn                       0.32.0
validators                    0.34.0
vlmeval                       0.1.0       /srv/scratch/CRUISE/Breeze/Project/Llava/COMP9444_Llava/benchmark/VLMEvalKit
wavedrom                      2.0.3.post3
wcwidth                       0.2.13
websockets                    11.0.3
wheel                         0.44.0
XlsxWriter                    3.2.0
xxhash                        3.5.0
yarl                          1.17.1
zipp                          3.20.2
Wed Nov 13 02:34:45 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40S                    Off |   00000000:21:00.0 Off |                    0 |
| N/A   31C    P0             88W /  350W |       1MiB /  46068MiB |      3%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.07s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.25s/it]
Using the latest cached version of the dataset since Trelis/chess_pieces couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'default' at /srv/scratch/CRUISE/Breeze/.cache/huggingface/datasets/Trelis___chess_pieces/default/0.0.0/332c876be2f430b530d18deb1895308192bcf1ba (last modified on Mon Nov 11 07:52:16 2024).
Processing examples:   0%|          | 0/3 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Processing examples:  33%|███▎      | 1/3 [00:03<00:06,  3.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Processing examples:  67%|██████▋   | 2/3 [00:04<00:02,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Processing examples: 100%|██████████| 3/3 [00:06<00:00,  1.80s/it]Processing examples: 100%|██████████| 3/3 [00:06<00:00,  2.02s/it]
Ground Truth: A white bishop with a white knight.
Model Output: The image features two chess pieces: a white rook and a black king. Both pieces are made of wood and placed on a wooden surface.
Batch BLEU: 0.0000, Batch BERTScore F1: 0.8690
Ground Truth: A black rook, a black bishop and a white bishop all standing together.
Model Output: The chess pieces in the image include a white king, a black queen, a white rook, a black bishop, and a white pawn. These pieces are arranged on a wooden chessboard.
Batch BLEU: 0.1319, Batch BERTScore F1: 0.8813
Ground Truth: A black knight and a white rook. There is also half of what appears to be a black pawn to the side.
Model Output: In the image, there are two chess pieces on a wooden table: a white rook and a black rook.
Batch BLEU: 0.0000, Batch BERTScore F1: 0.8850
Accuracy: 0.00%
Average BLEU Score: 0.0440
Average BERTScore Precision: 0.8686
Average BERTScore Recall: 0.8885
Average BERTScore F1: 0.8784
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.16s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.29s/it]
Using the latest cached version of the dataset since Trelis/chess_pieces couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'default' at /srv/scratch/CRUISE/Breeze/.cache/huggingface/datasets/Trelis___chess_pieces/default/0.0.0/332c876be2f430b530d18deb1895308192bcf1ba (last modified on Mon Nov 11 07:52:16 2024).
Processing examples:   0%|          | 0/3 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
----- nvidia-smi output at Wed 13 Nov 2024 02:35:45 AEDT -----
Wed Nov 13 02:35:45 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40S                    Off |   00000000:21:00.0 Off |                    0 |
| N/A   41C    P0            239W /  350W |   18405MiB /  46068MiB |     96%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    303526      C   ...eze/miniconda/envs/llava/bin/python      18396MiB |
+-----------------------------------------------------------------------------------------+
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Processing examples:  33%|███▎      | 1/3 [00:07<00:15,  7.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Processing examples:  67%|██████▋   | 2/3 [00:11<00:05,  5.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Processing examples: 100%|██████████| 3/3 [00:15<00:00,  4.92s/it]Processing examples: 100%|██████████| 3/3 [00:15<00:00,  5.28s/it]
Ground Truth: A white bishop with a white knight.
Model Output: The image shows a chess set with the following pieces:

- A white king, which is the central piece in the center of the image.
- A white queen, which is the second piece from the king, located to the right of the king.
- A white rook, which is the third piece from the king, located to the right of the queen.
- A white bishop, which is the fourth piece from the king, located to the right of the rook.
- A white knight, which is the fifth piece from the king, located to the right of the bishop.
- A white pawn, which is the sixth piece from the king, located to the right of the knight.

The pieces are made of a light-colored material, possibly plastic or a similar synthetic material, and they appear to be a simple, possibly educational or novelty set, given their simplistic design and the absence of intricate details or engravings. The pieces are placed on a flat, light-colored surface, which could be a table or a similar flat, horizontal object.
Batch BLEU: 0.0000, Batch BERTScore F1: 0.8155
Ground Truth: A black rook, a black bishop and a white bishop all standing together.
Model Output: The image shows a set of chess pieces laid out on a flat surface. From left to right, there is a **black Rook**, a **black Knight**, a **white Rook**, a **white Knight**, and a **white Bishop**. The pieces are in a somewhat disorganized arrangement, with the Rooks at the ends and the Knights and Bishop in the middle. The pieces appear to be of a standard design, with the Rooks being the tallest and the Bishop being the shortest.
Batch BLEU: 0.0000, Batch BERTScore F1: 0.8461
Ground Truth: A black knight and a white rook. There is also half of what appears to be a black pawn to the side.
Model Output: The image shows a chess set with the following pieces:

- A white king, which is the central piece in the image.
- A black rook, which is the piece to the left of the king.
- A white bishop, which is the piece to the right of the king.
- A black knight, which is the piece to the right of the rook.
- A white pawn, which is the piece to the left of the rook.

The pieces are placed on a flat, light-colored surface, and the lighting suggests an indoor setting. The pieces appear to be of a standard chess set, with the king being the most prominent due to its central position.
Batch BLEU: 0.0000, Batch BERTScore F1: 0.8470
Accuracy: 0.00%
Average BLEU Score: 0.0000
Average BERTScore Precision: 0.8023
Average BERTScore Recall: 0.8733
Average BERTScore F1: 0.8362
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.21s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.30s/it]
Using the latest cached version of the dataset since Trelis/chess_pieces couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'default' at /srv/scratch/CRUISE/Breeze/.cache/huggingface/datasets/Trelis___chess_pieces/default/0.0.0/332c876be2f430b530d18deb1895308192bcf1ba (last modified on Mon Nov 11 07:52:16 2024).
Processing examples:   0%|          | 0/3 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Processing examples:  33%|███▎      | 1/3 [00:04<00:08,  4.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Processing examples:  67%|██████▋   | 2/3 [00:07<00:03,  4.00s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Processing examples: 100%|██████████| 3/3 [00:11<00:00,  3.60s/it]Processing examples: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]
Ground Truth: A white bishop with a white knight.
Model Output: The image shows two chess pieces on a wooden surface. On the left, there is a white rook with a black square base, and on the right, there is a black rook with a white square base. Both pieces appear to be made of a light-colored material, possibly plastic or wood, and have a smooth finish. The rooks are positioned diagonally from each other, with the white rook on the left and the black rook on the right.
Batch BLEU: 0.0000, Batch BERTScore F1: 0.8397
Ground Truth: A black rook, a black bishop and a white bishop all standing together.
Model Output: The image shows a set of chess pieces on a wooden surface. From left to right, there is a black rook, a black bishop, a white rook, and a white bishop. The pieces are arranged in a line, with the black rook and bishop on the left and the white rook and bishop on the right. The pieces appear to be made of a dark wood, possibly ebony, for the black pieces and a lighter wood, possibly light-colored wood or plastic, for the white pieces. The image is taken from a perspective that casts a shadow on the surface, indicating a light source from the upper left side.
Batch BLEU: 0.0558, Batch BERTScore F1: 0.8546
Ground Truth: A black knight and a white rook. There is also half of what appears to be a black pawn to the side.
Model Output: The image shows a chessboard with two pieces on it. On the left, there is a black rook, which is a piece that can move horizontally or vertically. On the right, there is a white queen, which is the most powerful piece in the game of chess, able to move in any direction along a rank, file, or diagonal. The chessboard itself is not fully visible, but it appears to be a standard wooden chessboard with alternating light and dark squares.
Batch BLEU: 0.0383, Batch BERTScore F1: 0.8568
Accuracy: 0.00%
Average BLEU Score: 0.0313
Average BERTScore Precision: 0.8207
Average BERTScore Recall: 0.8823
Average BERTScore F1: 0.8504
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.13s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.08s/it]----- nvidia-smi output at Wed 13 Nov 2024 02:36:46 AEDT -----
Wed Nov 13 02:36:46 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40S                    Off |   00000000:21:00.0 Off |                    0 |
| N/A   40C    P0             82W /  350W |   18405MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    303526      C   ...eze/miniconda/envs/llava/bin/python      18396MiB |
+-----------------------------------------------------------------------------------------+
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.27s/it]
Using the latest cached version of the dataset since Trelis/chess_pieces couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'default' at /srv/scratch/CRUISE/Breeze/.cache/huggingface/datasets/Trelis___chess_pieces/default/0.0.0/332c876be2f430b530d18deb1895308192bcf1ba (last modified on Mon Nov 11 07:52:16 2024).
Processing examples:   0%|          | 0/3 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Processing examples:  33%|███▎      | 1/3 [00:03<00:07,  3.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Processing examples:  67%|██████▋   | 2/3 [00:05<00:02,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Processing examples: 100%|██████████| 3/3 [00:07<00:00,  2.39s/it]Processing examples: 100%|██████████| 3/3 [00:07<00:00,  2.55s/it]
Ground Truth: A white bishop with a white knight.
Model Output: The image shows a wooden chess set on a wooden surface. There is a white knight, a black pawn, a white pawn, a black queen, and a white bishop. The white knight is standing on top of the white pawn, and the white bishop is standing on top of the black pawn. The black queen is also visible in the background.
Batch BLEU: 0.0000, Batch BERTScore F1: 0.8483
Ground Truth: A black rook, a black bishop and a white bishop all standing together.
Model Output: The image shows a white king, a white queen, a white knight, a white bishop, and a black pawn on a wooden chessboard.
Batch BLEU: 0.0000, Batch BERTScore F1: 0.8850
Ground Truth: A black knight and a white rook. There is also half of what appears to be a black pawn to the side.
Model Output: The image shows a wooden chessboard with several pieces on it. There is a white knight, a black pawn, a white pawn, a black queen, and a white bishop. The pieces are arranged on the board, and the background is blurred, making it difficult to discern the rest of the chessboard.
Batch BLEU: 0.0000, Batch BERTScore F1: 0.8761
Accuracy: 0.00%
Average BLEU Score: 0.0000
Average BERTScore Precision: 0.8560
Average BERTScore Recall: 0.8843
Average BERTScore F1: 0.8698
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.20s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.11s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.30s/it]
Using the latest cached version of the dataset since Trelis/chess_pieces couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'default' at /srv/scratch/CRUISE/Breeze/.cache/huggingface/datasets/Trelis___chess_pieces/default/0.0.0/332c876be2f430b530d18deb1895308192bcf1ba (last modified on Mon Nov 11 07:52:16 2024).
Processing examples:   0%|          | 0/3 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Processing examples:  33%|███▎      | 1/3 [00:01<00:03,  1.86s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Processing examples:  67%|██████▋   | 2/3 [00:03<00:01,  1.65s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Processing examples: 100%|██████████| 3/3 [00:04<00:00,  1.51s/it]Processing examples: 100%|██████████| 3/3 [00:04<00:00,  1.60s/it]
Ground Truth: A white bishop with a white knight.
Model Output: "A white bishop and a black knight."
Batch BLEU: 0.0000, Batch BERTScore F1: 0.9322
Ground Truth: A black rook, a black bishop and a white bishop all standing together.
Model Output: "A white bishop and a black knight are visible in the image, with the bottom of some black chess pieces in the background."
Batch BLEU: 0.0000, Batch BERTScore F1: 0.8847
Ground Truth: A black knight and a white rook. There is also half of what appears to be a black pawn to the side.
Model Output: "A white bishop and a black knight are visible, with the bottom of a white king and a black rook partially shown in the background."
Batch BLEU: 0.0000, Batch BERTScore F1: 0.9077
Accuracy: 0.00%
Average BLEU Score: 0.0000
Average BERTScore Precision: 0.9016
Average BERTScore Recall: 0.9150
Average BERTScore F1: 0.9082
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.26s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.13s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.32s/it]
Using the latest cached version of the dataset since Trelis/chess_pieces couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'default' at /srv/scratch/CRUISE/Breeze/.cache/huggingface/datasets/Trelis___chess_pieces/default/0.0.0/332c876be2f430b530d18deb1895308192bcf1ba (last modified on Mon Nov 11 07:52:16 2024).
----- nvidia-smi output at Wed 13 Nov 2024 02:37:46 AEDT -----
Wed Nov 13 02:37:46 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40S                    Off |   00000000:21:00.0 Off |                    0 |
| N/A   40C    P0             87W /  350W |   18619MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    303526      C   ...eze/miniconda/envs/llava/bin/python      18610MiB |
+-----------------------------------------------------------------------------------------+
Processing examples:   0%|          | 0/3 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Processing examples:  33%|███▎      | 1/3 [00:03<00:07,  3.51s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Processing examples:  67%|██████▋   | 2/3 [00:06<00:03,  3.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Processing examples: 100%|██████████| 3/3 [00:09<00:00,  3.00s/it]Processing examples: 100%|██████████| 3/3 [00:09<00:00,  3.13s/it]
Ground Truth: A white bishop with a white knight.
Model Output: The image shows a wooden chess set with the following pieces visible:

- A white king.
- A white queen.
- A white knight.
- A black pawn.
- A black rook.

The pieces are arranged on a wooden surface, and the focus is on the king and the queen, which are standing close together. The other pieces are partially visible in the background.
Batch BLEU: 0.0000, Batch BERTScore F1: 0.8482
Ground Truth: A black rook, a black bishop and a white bishop all standing together.
Model Output: The image shows a wooden chess set with the following pieces visible:

- A white knight.
- A black pawn and a black rook.
- A white pawn, a black queen, and a white knight. The white knight is partially obscured by the black queen.
- A white king, a white queen, and a black king. The white king and queen are partially obscured by the black king.
- A white bishop with the bottom of some black chess pieces in the background.
Batch BLEU: 0.0000, Batch BERTScore F1: 0.8540
Ground Truth: A black knight and a white rook. There is also half of what appears to be a black pawn to the side.
Model Output: The image shows a wooden chessboard with several pieces on it. There is a white knight, a black pawn, a white pawn, a black queen, and a white bishop. The pieces are arranged in a way that suggests a game in progress, with the white knight and black pawn in the foreground, the white pawn and black queen in the middle, and the white bishop in the background.
Batch BLEU: 0.0000, Batch BERTScore F1: 0.8714
Accuracy: 0.00%
Average BLEU Score: 0.0000
Average BERTScore Precision: 0.8365
Average BERTScore Recall: 0.8806
Average BERTScore F1: 0.8579
Wed Nov 13 02:38:00 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40S                    Off |   00000000:21:00.0 Off |                    0 |
| N/A   42C    P0             93W /  350W |       1MiB /  46068MiB |      3%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
